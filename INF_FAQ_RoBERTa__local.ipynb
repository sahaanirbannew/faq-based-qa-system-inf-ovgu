{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "INF_FAQ_RoBERTa _local.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahaanirbannew/faq-based-qa-system-inf-ovgu/blob/Roberta_local/INF_FAQ_RoBERTa__local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lgO9Uz74osR"
      },
      "source": [
        "# FAQ based Q&A system prototype\n",
        "Version: 1.0 <br>\n",
        "Developer: Anirban Saha. <br>\n",
        "\n",
        "**Drawbacks:**\n",
        "\n",
        "\n",
        "*   Uses ElasticSearch Cloud. It is free version. It expires by 20.03.2021.\n",
        "*   A few approaches are rudimentary and primitive.\n",
        "\n",
        "**Future work includes:**\n",
        "\n",
        "\n",
        "*   Using ElasticSearch in local machine.\n",
        "*   Scraping FIN websites, create knowledge base.\n",
        "\n",
        "*   Making a textual entailment model. It would take user's query, a FAQ question and see if the query entails the existing FAQ question. If yes, then it should return the answer.\n",
        "\n",
        "\n",
        "*   Bettering the approach towards understanding intent - \"Asking for link\".\n",
        "*   Creation of better knowledge.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxQy553VDBB_"
      },
      "source": [
        "# Importing and downloading stuff\n",
        "For this section, please run the cells individually. Also please take a note of the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YM53VR42GCF"
      },
      "source": [
        "import spacy\n",
        "#!python -m spacy download en_core_web_lg\n",
        "# After downloading en_core_eb_lg, please restart runtime."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeqIWZfitFsk",
        "outputId": "32698ba7-249a-42cb-83e5-61523f6bdf7b"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install elasticsearch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 19.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=c4d19b6c2e21fbfc7b307354ace1a0551c0cf269e2a9f3f6ab53a6adc556c465\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/68/76c5d46cc6a48fddb759f585bc8728caa11bfc9b812ce6705fc5f99beab2/elasticsearch-7.11.0-py2.py3-none-any.whl (325kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF5t7dVzAA7N"
      },
      "source": [
        "import pandas as pd \n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "from elasticsearch import helpers, Elasticsearch \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cxvdkk60lKD"
      },
      "source": [
        "# Loading the datasets.\n",
        "Loads the datasets to local variable. Nothing fancy. Pretty straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmtf63MFyzvV"
      },
      "source": [
        "\"\"\"\n",
        "Description: Takes download url, filename. Downloads it. Saves. \n",
        "\"\"\"\n",
        "def download_file(download_url,filename):\n",
        "  response = requests.get(download_url)\n",
        "  with open(filename+'.csv', 'wb') as f:\n",
        "      f.write(response.content)\n",
        "  return filename+'.csv'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRuBVcBK9QI"
      },
      "source": [
        "\"\"\"\n",
        "Description: loads data from file.\n",
        "\"\"\"\n",
        "def load_links(file_path): \n",
        "  links = pd.read_csv(file_path) \n",
        "  return links"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPdhgVqlhira"
      },
      "source": [
        "def load_faq(file_path):   \n",
        "  faq_data = pd.read_csv(file_path) \n",
        "  return faq_data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1IV_6CGzE2K"
      },
      "source": [
        "file_path_faq = download_file(\"https://www.anirbansaha.com/wp-content/uploads/2021/03/faq.csv\",\"faq\")\n",
        "file_path_links = download_file(\"https://www.anirbansaha.com/wp-content/uploads/2021/03/links.csv\",\"links\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01quPyUuuL28"
      },
      "source": [
        "import csv \r\n",
        "import json \r\n",
        "\r\n",
        "def csv_to_json(csvFilePath, jsonFilePath):\r\n",
        "    jsonArray = []\r\n",
        "      \r\n",
        "    #read csv file\r\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf: \r\n",
        "        #load csv file data using csv library's dictionary reader\r\n",
        "        csvReader = csv.DictReader(csvf) \r\n",
        "\r\n",
        "        #convert each csv row into python dict\r\n",
        "        for row in csvReader: \r\n",
        "            #add this python dict to json array\r\n",
        "            jsonArray.append(row)\r\n",
        "  \r\n",
        "    #convert python jsonArray to JSON String and write to file\r\n",
        "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \r\n",
        "        jsonString = json.dumps(jsonArray, indent=4)\r\n",
        "        jsonf.write(jsonString)\r\n",
        "          \r\n",
        "csvFilePath = file_path_links\r\n",
        "jsonFilePath = r'links.json'\r\n",
        "csv_to_json(csvFilePath, jsonFilePath)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oM31UxmzhkM"
      },
      "source": [
        "\"\"\"\r\n",
        "Description: Loads the data and saves it in local variable for use. \r\n",
        "\"\"\"\r\n",
        "links_df = load_links(file_path_links)\r\n",
        "faq_data_df = load_faq(file_path_faq)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsOhBV3B9_oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9a50f8-9f31-41a2-a09b-77df586810c8"
      },
      "source": [
        "\"\"\"\n",
        "Description: Loads the data and saves it in local variable for use. \n",
        "\"\"\"\n",
        "with open('faq.json',encoding=\"utf8\") as f:\n",
        "    faq_data = json.load(f)\n",
        "f.close()\n",
        "print(len(faq_data))\n",
        "\n",
        "with open('links.json',encoding=\"utf8\") as f:\n",
        "    links = json.load(f)\n",
        "f.close()\n",
        "print(len(links))\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF0qpypu0rMv"
      },
      "source": [
        "# Connecting to ElasticSearch Cloud\n",
        "Connects to the ElasticSearch Cloud. If you are replicating it, please take a note of the base endpoint. <br>\n",
        "\n",
        "\n",
        "*   Create account, do stuff, following this: https://youtu.be/mIHYcxe70fc\n",
        "*   I have converted csv files to json, uploaded to \"documents\" section of ElasticSearch Enterprise Search.\n",
        "*   CSV --> Json: https://csvjson.com/ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI6p1DZXqZcI"
      },
      "source": [
        "config = { \"appsearch\":{\n",
        "              \"base_endpoint\":\"2aa49783e67340e585db1d090ca796d0.ent-search.eastus2.azure.elastic-cloud.com/api/as/v1\",\n",
        "              \"api_key\":\"private-3dd7pg3n5dr5nmitvn8647i6\"\n",
        "              }\n",
        "          }\n",
        "#note: do not forget to add the \"/api/as/v1\" at the end of the endpoint. \n",
        "client = Client(\n",
        "   base_endpoint=config['appsearch']['base_endpoint'],\n",
        "   api_key=config['appsearch']['api_key'],\n",
        "   use_https=True)\n",
        "engine_name = \"inf-faq\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C5FItt205ZV"
      },
      "source": [
        "# The set of Questions we would primarily test this with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRJovAG_uOhx"
      },
      "source": [
        "questions = [\"Give me the link to the mentors\",\n",
        "             \"Where can I find the podcasts about student jobs?\",\n",
        "             \"Who can sign a care-of letter?\",\n",
        "             \"If I lose my student id card, where should i report?\",\n",
        "             \"What documents do i need for visa extension?\",\n",
        "             \"Where can I print documents?\",\n",
        "             \"which is the computer science faculty building?\",\n",
        "             \"when should we apply for jobs?\",\n",
        "             \"When will I get a student card?\",\n",
        "             \"Where can I buy coffee?\",\n",
        "             \"What’s a “care-of letter”?\",\n",
        "             \"What is cold rent?\",\n",
        "             \"How can I find accommodation?\",\n",
        "             \"What is FIN?\",\n",
        "             \"Where will I find English speaking Doctors?\",\n",
        "             \"Where will i find doctors who speak English?\",\n",
        "             \"how should i choose my subjects?\",\n",
        "             \"Where can i get a list of subjects offered by the university?\",\n",
        "             \"Which health insurance should i take?\"\n",
        "             ]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWWtrBOt1Fxr"
      },
      "source": [
        "# The Huggingface RoBERTa stuff\n",
        "Nothing fancy. Code is self explanatory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOALMXp6tUzS",
        "outputId": "310d2f29-cedc-4b4d-cde1-066e62252408"
      },
      "source": [
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tE2API0krpX"
      },
      "source": [
        "def fetching_searchterm(passage):  \n",
        "  passage = str(passage).replace(\"**\",\",\")\n",
        "  query = \"What is he searching for?\"\n",
        "  QA_input = {\n",
        "    'question': query,\n",
        "    'context': passage\n",
        "  }\n",
        "  ans = nlp(QA_input)  \n",
        "  if ans['score']>0.32:\n",
        "    return(ans)\n",
        "  else:\n",
        "    return\"\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkNn9xIpuUCu"
      },
      "source": [
        "def ask_question_huggingfaceRoberta(user_question, passage):  \n",
        "  passage = str(passage).replace(\"**\",\",\")\n",
        "  QA_input = {\n",
        "    'question': user_question,\n",
        "    'context': passage\n",
        "  }\n",
        "  ans = nlp(QA_input)  \n",
        "  return(ans)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QogcGr0k1Lff"
      },
      "source": [
        "# The NLP stuff\n",
        "Again, nothing fancy. Takes two sentences, calculates similarity (word movers distance). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRR-ISNprlgC"
      },
      "source": [
        "nlp_sim = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue2Gn_tLqKtK"
      },
      "source": [
        "def similarity_spacy(question1, question2):\n",
        "    text1 = nlp_sim(question1)\n",
        "    text2 = nlp_sim(question2)\n",
        "    simi = text1.similarity(text2)\n",
        "    return simi "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd2RaYNO1Xe4"
      },
      "source": [
        "# The Q&A stuff\n",
        "**find_probable_answer_from_faq**<br>\n",
        "Takes user query, tries to find closest match in the FAQ questions. If there is a close match, it returns the answer.\n",
        "\n",
        "**understanding_question_link**<br>\n",
        "Tries to understand if the user is asking for a link. This piece of code is far from being perfect. I need help in this.\n",
        "\n",
        "**get_link_answer**<br>\n",
        "If we understand that the user is asking for a link, we check if we have the link. If we have, we return the link.\n",
        "\n",
        "**answer_query**<br>\n",
        "This is the main fancy program. I am adding inline comments to the code. Please check that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtjcpDU0rMX0"
      },
      "source": [
        "def find_probable_answer_from_faq(query):\n",
        "  selected_question = \"\"\n",
        "  answer = \"\"\n",
        "  highest_score = 0\n",
        "  for index, row in faq_data_df.iterrows(): \n",
        "    similarity_score = similarity_spacy(row['question'], query) \n",
        "    if similarity_score>highest_score:\n",
        "      answer = row['answers']\n",
        "      question= row['question']\n",
        "      highest_score = similarity_score\n",
        "  \n",
        "  if highest_score>0.9446: #Do not change this number.  \n",
        "    return answer\n",
        "  else: \n",
        "    return \"\" "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEkjyP4fJL0U"
      },
      "source": [
        "def preprocess_for_searchterm(query):\n",
        "  query = query.replace(\"catalogue\", \"catalog\")\n",
        "  query = query.replace(\"examination\", \"exam\")\n",
        "  query = query.replace(\"module handbook\", \"modulehandbook\")\n",
        "  query = query.replace(\"module hand book\", \"modulehandbook\")\n",
        "  query = query.replace(\"module catalog\", \"modulehandbook\")\n",
        "  query = query.replace(\"modulhandbuch\", \"modulehandbook\")\n",
        "  query = query.replace(\"si@fin videos\", \"SI@FIN_videos\")\n",
        "  query = query.replace(\"exam office\", \"exam_office\")\n",
        "  query = query.replace(\"comic strips\", \"comics\")\n",
        "  return query"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jILQQDY7pYns"
      },
      "source": [
        "def fetching_interest(passage):  \n",
        "  query = \"What is he searching for?\"\n",
        "  QA_input = {\n",
        "    'question': query,\n",
        "    'context': passage\n",
        "  }\n",
        "  ans = nlp(QA_input) \n",
        "  if ans['score']>0.32:\n",
        "    return(ans['answer'])\n",
        "  else:\n",
        "    return\"\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRIZoHJ27Z_c"
      },
      "source": [
        "def understanding_question_link(query): \n",
        "  query = query.lower()\n",
        "  query = preprocess_for_searchterm(query) \n",
        "  search_term = fetching_interest(query)\n",
        "  return search_term\n",
        "\n",
        "  if search_term == \"\":\n",
        "    #do all the tamasha -_- \n",
        "    stopwords = [\"a\", \"an\", \"the\"]\n",
        "    asking_for_link = False \n",
        "    asking = ['give','send', \"what is\", \"search\"]\n",
        "    for ask in asking:\n",
        "      if ask in query.split(\"link\")[0]:\n",
        "        asking_for_link = True #wrong logic\n",
        "\n",
        "    if asking_for_link == True: \n",
        "      case = 0\n",
        "      if \"link to\" in query or \"link of\" in query or \"link for\" in query: case = 1\n",
        "      if \"'s link\" in query or \"s link\" in query: case = 2\n",
        "\n",
        "      \n",
        "\n",
        "      if case == 1:\n",
        "        text_tokens = query.split(\"link\")[1].strip().split(\" \")\n",
        "        tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
        "        return tokens_without_sw[1]\n",
        "      \n",
        "      if case == 2:\n",
        "        topic = query.split(\"link\")[0].strip().split(\" \")[-1]\n",
        "        topic = topic.replace(\"'s\",\"\")\n",
        "        return topic\n",
        "    \n",
        "    return \"not found\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GTFXY2pjOdlN",
        "outputId": "aeb42b4f-befd-426c-fe6a-df1494587af2"
      },
      "source": [
        "understanding_question_link(\"search a list of courses offered by the university?\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM4C7JTxMSXi"
      },
      "source": [
        "def get_link_answer(query):\n",
        "  topic = understanding_question_link(query) \n",
        "  try:\n",
        "    if len(links_df[links_df['Description'].str.match(topic)])>0:\n",
        "      return links_df[links_df['Description'].str.match(topic)].iloc[0]['Link']\n",
        "  except:\n",
        "    status = \"pending\"\n",
        "  return \"\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Or75YJ8yJDz"
      },
      "source": [
        "def preprocess(query):\n",
        "  changes = {\"course list\":\"module handbook\",\n",
        "             \"list of courses\":\"module handbook\",\n",
        "             \"subject\":\"course\",\n",
        "             \"website link\":\"link\",\n",
        "             \"website\":\"link\",\n",
        "             \"si@fin\":\"SI@FIN\",\n",
        "             \"SI@FIN videos\":\"SI@FIN_videos\",\n",
        "             \"course videos\":\"SI@FIN_videos\"\n",
        "             }\n",
        "  for key in changes:\n",
        "    query = query.replace(key,changes[key]) \n",
        "  return query"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cudw0bg1QhXZ"
      },
      "source": [
        "def understand_intent(query, num):\n",
        "  query = query.lower()\n",
        "  intent_search = {\"where is the\":\"search\",\n",
        "                    \"where can I find the\":\"search\",\n",
        "                    \"where will i find\":\"search\",\n",
        "                    \"where can i get\":\"search\",\n",
        "                    \"where will i get\":\"search\" \n",
        "                  }\n",
        "  searchterm = fetching_interest(query) \n",
        "  if (searchterm) and num == 0:\n",
        "    return \"search\" \n",
        "  for key in intent_search:\n",
        "    if key in query and num == 0: \n",
        "      return intent_search[key]\n",
        "    if key in query and num == 1:  \n",
        "      return query.replace(key, intent_search[key]) \n",
        "  if num==0: return \"\"\n",
        "  if num==1: return query"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_W_35sD6s5Y",
        "outputId": "dfda2d99-6380-4e95-e026-0d8d5a0774a9"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\r\n",
        "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\r\n",
        "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\r\n",
        "sudo chown -R daemon:daemon elasticsearch-7.9.2/\r\n",
        "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hloNrsCr8nFp",
        "outputId": "3453763b-c589-4ed9-ac73-28248a866be8"
      },
      "source": [
        "%%bash --bg\r\n",
        "\r\n",
        "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGDoqVjf8qHb"
      },
      "source": [
        "# Sleep for few seconds to let the instance start.\r\n",
        "import time\r\n",
        "time.sleep(20)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJd6EPRN8zLl",
        "outputId": "aecb0d06-6a7c-4203-c760-66c89c4c07ca"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "ps -ef | grep elasticsearch"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root         748     746  0 19:52 ?        00:00:00 sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch\n",
            "daemon       749     748 30 19:52 ?        00:00:16 /content/elasticsearch-7.9.2/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms1g -Xmx1g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-16507916463483370030 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.9.2 -Des.path.conf=/content/elasticsearch-7.9.2/config -Des.distribution.flavor=oss -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.9.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n",
            "root         988     986  0 19:53 ?        00:00:00 grep elasticsearch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bthco4fw79A8",
        "outputId": "ce2a471f-c353-4f2a-81e1-980bdeb47b29"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "curl -sX GET \"localhost:9200/\""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\" : \"b0e5b9a78df1\",\n",
            "  \"cluster_name\" : \"elasticsearch\",\n",
            "  \"cluster_uuid\" : \"Ca6z0MZzTcye3ueiPAAI_A\",\n",
            "  \"version\" : {\n",
            "    \"number\" : \"7.9.2\",\n",
            "    \"build_flavor\" : \"oss\",\n",
            "    \"build_type\" : \"tar\",\n",
            "    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n",
            "    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n",
            "    \"build_snapshot\" : false,\n",
            "    \"lucene_version\" : \"8.6.2\",\n",
            "    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n",
            "    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n",
            "  },\n",
            "  \"tagline\" : \"You Know, for Search\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v56j88ep5E8U"
      },
      "source": [
        "for i in range(len(faq_data)):\r\n",
        "    es.index(index=\"fin_faq\", doc_type=\"document\", id=i, body=faq_data[i])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S0GYWZI9WeG"
      },
      "source": [
        "def answer_search(query):\r\n",
        "    body = {\r\n",
        "    \"from\":0,\r\n",
        "    \"size\":1,\r\n",
        "    \"query\": {\r\n",
        "        \"match\": {\r\n",
        "            \"question\":query\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "    res = es.search(index=\"fin_faq\", body=body)\r\n",
        "    return res"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpEi1GOnwZ_"
      },
      "source": [
        "def answer_query(query):\n",
        "  #basic preprocessing. Very primitive work. Needs improvement. \n",
        "  query = preprocess(query) \n",
        "  intent = understand_intent(query, 0) \n",
        "  if (intent):print(\"intent: \"+intent)\n",
        "  #Checks for exact matches with the questions in FAQ.\n",
        "  #Most probably, it will not be the case. \n",
        "  try:\n",
        "    result_exact_match = faq_data_df[faq_data_df['question'].str.match(query)]\n",
        "    if len(result_exact_match)>0: \n",
        "      return \"found by exact match.\", result_exact_match.iloc[0]['answers'].replace(\"**\",\",\")\n",
        "  except:\n",
        "    status = \"paining.\" #because i do not know what to do. LOL.\n",
        "  \n",
        "  #Checks if the user might be wanting a link as an answer.\n",
        "  #If yes, and if the link exists, then it returns the link.\n",
        "  #The exhaustive the list of links, the better the responses.\n",
        "  #Currently, this is in a primitive state. \n",
        "  if \"link\" in query or intent == \"search\":\n",
        "    temp_query = understand_intent(query, 1)\n",
        "    answer = get_link_answer(temp_query)\n",
        "    if len(answer)>0:\n",
        "      return \"Retrieved link.\", answer  \n",
        "\n",
        "  #Tries to find the best match in FAQ queries by ElasticSearch.\n",
        "  #If yes, returns the entire passage as answer.\n",
        "\n",
        "\n",
        "  data = answer_search(query) \n",
        "  score_passage = data['hits']['hits'][0]['_score']\n",
        "  if score_passage > 100: \n",
        "    return \"found by similarity match by Elastic Search.\", data['hits']['hits'][0]['_source']['answers'].replace(\"**\",\",\") \n",
        "\n",
        "\n",
        "  #Checks for the closest match in FAQ questions.\n",
        "  #In case there is a match, it returns the answer. \n",
        "  answer = find_probable_answer_from_faq(query) \n",
        "  if len(answer) > 0: \n",
        "    return \"found by similarity match.\", answer.replace(\"**\",\",\") + \"\\nInfo: (This answer is retrieved using similarity search from FAQ. In case this is not the answer you are looking for, please rephrase your question or ask a mentor.)\"\n",
        "  \n",
        "  #This is the last case and I suppose this would be the most used case.\n",
        "  #If the question is not matched, it will try searching for an answer from the literature.\n",
        "  #The responses of this is not very accurate always. We do not want to give inaccurate answers to users.\n",
        "  #To avoid this, we should do the following:\n",
        "  # * keep adding questions to the repository\n",
        "  # * write exhaustive answers in simple english sentences. Use less reference words \n",
        "  #.    like he, she, it, this, that. \n",
        "  answer = ask_question_huggingfaceRoberta(query, data['hits']['hits'][0]['_source']['answers'])\n",
        "  if answer['score'] > 0.01:\n",
        "    return \"found by RoBERTa.\", answer['answer'].replace(\"**\",\",\") + \"\\nInfo:(This answer might be wrong. Please consult with a mentor or the faculty.)\"\n",
        "  \n",
        "  return \"please consult a mentor.\"+answer_query(\"give me the link to mentors\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKDdUg2A1gx7"
      },
      "source": [
        "# The Moment of Truth.\n",
        "It uses the existing set of questions. Tries to find answers to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVdCdWEI0Uvk",
        "outputId": "2b9dedcb-ffc1-421f-f1f9-6f82e97e7ac8"
      },
      "source": [
        "# For the purpose of this notebook, we have a fixed set of questions.\n",
        "# Ideally user_question is a query by the user. \n",
        "for query in questions:\n",
        "  print('Users question: ' + query)\n",
        "  explanation, final_answer = answer_query(query) \n",
        "  print(final_answer) \n",
        "  print(\"*\"*50)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Users question: Give me the link to the mentors\n",
            "intent: search\n",
            "https://www.inf.ovgu.de/inf/en/Study/Being+a+student/Incoming/Mentors-p-5082.html\n",
            "**************************************************\n",
            "Users question: Where can I find the podcasts about student jobs?\n",
            "portals\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: Who can sign a care-of letter?\n",
            "your friend\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: If I lose my student id card, where should i report?\n",
            "intent: search\n",
            "If you have lost your student card, please report it immediately at the Campus Service Center and apply for the new one. The application for a new student card comes with a fee. Follow the procedure as mentioned by the officials at the Campus Service Center.\n",
            "Info: (This answer is retrieved using similarity search from FAQ. In case this is not the answer you are looking for, please rephrase your question or ask a mentor.)\n",
            "**************************************************\n",
            "Users question: What documents do i need for visa extension?\n",
            "intent: search\n",
            "Passport with a valid visa, Certificate of residence\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: Where can I print documents?\n",
            "IT center\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: which is the computer science faculty building?\n",
            "intent: search\n",
            "rooms 147 and 427\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: when should we apply for jobs?\n",
            "mid-September\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: When will I get a student card?\n",
            "You should get your student card within a couple of days from enrolment. You can collect the student card from the Campus Service Center (G1). Once you get the student card or University card, please activate it at the Campus Service Center. The student card is your proof of being a student at this university, it is advisable to carry it with you especially while availing public transport in Magdeburg. You can find more details about the student card by clicking here.\n",
            "**************************************************\n",
            "Users question: Where can I buy coffee?\n",
            "Baracke\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: What’s a “care-of letter”?\n",
            "intent: search\n",
            "In case you do not find permanent accommodation and if you are staying with your friend for a short span of time till you get your own permanent accommodation, you can take a “c/o letter” or a “care-of letter” from your friend. Your friend might want to follow this format to make their care-of letter for you. The care-of letter should be printed and signed by your friend.\n",
            "**************************************************\n",
            "Users question: What is cold rent?\n",
            "intent: search\n",
            "no more than what you owe the landlord every month\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: How can I find accommodation?\n",
            "intent: search\n",
            "You can search for accommodation on various websites like Wobau, the campus tower, the campus house, Lorenz, MWG. You can also check wg-gesuct. You would get a list of the links on the FAQ page.\n",
            "**************************************************\n",
            "Users question: What is FIN?\n",
            "intent: search\n",
            "FIN is an acronym and stands for \"Fakultät für INformatik\" - Faculty of Computer Science. So basically its the nickname and old official name of the faculty, and since it is way easier to pronounce than INF, still very common.\n",
            "**************************************************\n",
            "Users question: Where will I find English speaking Doctors?\n",
            "intent: search\n",
            "Magdeburg\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: Where will i find doctors who speak English?\n",
            "intent: search\n",
            "Practise at Nord Park\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "**************************************************\n",
            "Users question: how should i choose my subjects?\n",
            "The exhaustive list of courses offered in the FIN are present in the Module Hand Book (check study documents), which provides a detailed description of courses. Alternatively, the courses offered each semester are available in the LSF (student portal). The list of courses offered for the summer and the winter semesters are mostly different from each other. You can find more information in the videos here. Once you find a course, you can see the details of the courses offered by going to their respective websites. The website link can also be found on the LSF.\n",
            "Info: (This answer is retrieved using similarity search from FAQ. In case this is not the answer you are looking for, please rephrase your question or ask a mentor.)\n",
            "**************************************************\n",
            "Users question: Where can i get a list of subjects offered by the university?\n",
            "intent: search\n",
            "https://www.inf.ovgu.de/inf/en/Study/Being+a+student/Incoming/Mentors-p-5082.html\n",
            "**************************************************\n",
            "Users question: Which health insurance should i take?\n",
            "intent: search\n",
            "Health insurance is mandatory for an international student in Germany. Some Insurance companies have their counters in the campus service centre during the registration period. You can also collect contact details of various insurance companies from the reception of the campus service centre. TO know more, visit: http://www.ikus.ovgu.de/ikus/en/Offer/Advice+_+Support/First+Steps/Step+4_+Health+insurance-p-110.html\n",
            "Info: (This answer is retrieved using similarity search from FAQ. In case this is not the answer you are looking for, please rephrase your question or ask a mentor.)\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGHmC3Y-vdlx"
      },
      "source": [
        "# Q&A System.\n",
        "Based on the FAQ for the incoming international students.<br>\n",
        "Please ask full sentence questions, the way you would talk to a human being. -_- <br> and relevant to your onboarding in OVGU.<br>\n",
        "Anirban Saha <br>\n",
        "05.03.2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0-wdMAc4gZZ",
        "outputId": "0e70144c-a6ff-4ca5-c711-ba3d18ee6ce9"
      },
      "source": [
        "query = input(\"query: \")\n",
        "explanation, answer = answer_query(query)\n",
        "print(answer)\n",
        "if explanation == \"found by RoBERTa.\": print(\"Explanation: \"+explanation)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: module list?\n",
            "intent: search\n",
            "Studiendokumente/Modulkatalog\n",
            "Info:(This answer might be wrong. Please consult with a mentor or the faculty.)\n",
            "Explanation: found by RoBERTa.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV0Ziwa2CHtI"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    }
  ]
}